apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: {{ .Release.Namespace }}
  namespace: {{ .Release.Namespace }}
spec:
  external:
    enable: {{ .Values.externalEnable }}
  cephVersion:
    image: {{ .Values.image.repository }}
    #image: ceph/ceph:v14.2.6
    allowUnsupported: {{ .Values.cephAllowUnsupported }}
  dataDirHostPath: {{ .Values.dataDirHostPath }}
  #dataDirHostPath: /var/lib/rook-cp
  skipUpgradeChecks: {{ .Values.skipUpgradeChecks }}
  # Whether or not continue if PGs are not clean during an upgrade
  continueUpgradeAfterChecksEvenIfNotHealthy: {{ .Values.continueUpgradeAfterChecksEvenIfNotHealthy }}
  # set the amount of mons to be started
  mon:
    count: {{ .Values.monCount }}
    allowMultiplePerNode: {{ .Values.allowMultiplePerNode }}
  dashboard:
    enabled: {{ .Values.dasbboardEnabled }}
    ssl: {{ .Values.dasbboardSSL }}
  monitoring:
    # requires Prometheus to be pre-installed
    enabled: {{ .Values.monitoringEnabled }}
    rulesNamespace: {{ .Release.Namespace }}
  network:
    # toggle to use hostNetwork
    hostNetwork: {{ .Values.hostNetwork }}
  rbdMirroring:
    workers: {{ .Values.rbdMirroring.workers }}
  crashCollector:
    disable: {{ .Values.crashCollector.disable }}
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
  # tolerate taints with a key of 'storage-node'.
{{ if eq .Values.placement.all true }}
 placement:
   all:
     nodeAffinity:
       requiredDuringSchedulingIgnoredDuringExecution:
         nodeSelectorTerms:
         - matchExpressions:
           - key: role
             operator: In
             values:
{{- range $roleValue := .Values.placement.nodeValues }}
             - {{ $roleValue }}
{{ end }}
     podAffinity:
     podAntiAffinity:
     tolerations:
     - key: storage-node
       operator: Exists
{{ end }}
#
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
# Monitor deployments may contain an anti-affinity rule for avoiding monitor
# collocation on the same node. This is a required rule when host network is used
# or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
# preferred rule with weight: 50.
#    osd:
#    mgr:
  annotations:
#    all:
#    mon:
#    osd:
# If no mgr annotations are set, prometheus scrape annotations will be set by default.
#   mgr:
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: "500m"
#        memory: "1024Mi"
#      requests:
#        cpu: "500m"
#        memory: "1024Mi"
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
#    prepareosd:
#    crashcollector:
  # The option to automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: false
#  priorityClassNames:
#    all: rook-ceph-default-priority-class
#    mon: rook-ceph-mon-priority-class
#    osd: rook-ceph-osd-priority-class
#    mgr: rook-ceph-mgr-priority-class
  storage: # cluster level storage configuration and selection
    useAllNodes: {{ .Values.useAllNodes }}
    useAllDevices: {{ .Values.useAllDevices }}
    #deviceFilter:
    config:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      # storeType: bluestore
      # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
      # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
      # journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
      osdsPerDevice: "1" # this value can be overridden at the node or device level
      # encryptedDevice: "true" # the default value for this option is "false"
# Cluster level list of directories to use for filestore-based OSD storage. If uncomment, this example would create an OSD under the dataDirHostPath.
    #directories:
    #- path: /var/lib/rook
# Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
{{ if eq .Values.useAllNodes false }}
{{- range $node := .Values.nodes }}
    nodes:
     - name: "{{ $node.name }}"
       devices:
{{- range $device := $node.devices }}
        - name: "{{ $device }}"
{{ end }} #range $device
{{ end }} # range $node
{{ end }} # if eq .Values.useAllNodes false
  disruptionManagement:
    # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
    # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
    # block eviction of OSDs by default and unblock them safely when drains are detected.
    managePodBudgets: false
    # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
    # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
    osdMaintenanceTimeout: 30
    # If true, the operator will create and manage MachineDisruptionBudgets to ensure OSDs are only fenced when the cluster is healthy.
    # Only available on OpenShift.
    manageMachineDisruptionBudgets: false
    # Namespace in which to watch for the MachineDisruptionBudgets.
    machineDisruptionBudgetNamespace: openshift-machine-api
